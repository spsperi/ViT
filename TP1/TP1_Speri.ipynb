{"cells":[{"cell_type":"markdown","source":["# TP1 - Vision Transformers\n","\n","Alumna: Sofía Speri"],"metadata":{"id":"mFmVzOqYQibz"}},{"cell_type":"markdown","metadata":{"id":"G1D9e9al0Pr1"},"source":["Recordatorio: si se está trabajando en un contenedor recien instalado, va a ser necesario instalar git-hub todas las librerias requeridas"]},{"cell_type":"markdown","metadata":{"id":"p8Emrfli0Pr6"},"source":["## Vision Transformer (ViT)\n","\n","El **Vision Transformer (ViT)** es un modelo innovador para la clasificación de imágenes que transforma las imágenes en secuencias de parches más pequeños, comúnmente de $16 \\times 16$ píxeles, como se describe en el paper de [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy)  \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\". Cada parche se considera como una \"palabra\" o \"token\" y se proyecta en un espacio de características. Mediante la incorporación de cifrados de posición y un token de clasificación, podemos aplicar un Transformer de manera convencional a esta secuencia, permitiendo su entrenamiento para tareas de clasificación de imágenes.\n","\n","## Embeddings y Cifrado de Posición\n","\n","### Embeddings\n","En el contexto de ViTs, las imágenes son primero divididas en pequeños bloques o patches (como si se cortaran en pequeños cuadrados). Estos patches se tratan como si fueran palabras en un modelo de lenguaje, y a cada uno se le asigna un embedding. Un embedding es simplemente una representación numérica del patch en un espacio de alta dimensión que captura características relevantes del mismo.\n","\n","Por ejemplo, si divides una imagen de 224x224 píxeles en 16x16 bloques, tendrás 196 patches en total. Cada uno de esos patches se convierte en un vector de embedding, que luego se alimenta al modelo Transformer.\n","\n","### Positional Embedding (Cifrado Posicional):\n","\n","El **embedding posicional** agrega información sobre la ubicación de cada parche en la imagen original. En el paper de Alexey Dosovitskiy et al., se evaluaron varias formas de codificar la información espacial con embebidos posicionales. Se observó que, aunque el uso de embebidos mejora el rendimiento en comparación con no usarlos, no hay diferencias significativas entre los métodos probados. Se concluyó que, dado que el modelo opera a nivel de parches en lugar de píxeles, la forma específica de codificar la posición es menos relevante.\n","## Proceso de Creación de Embeddings\n","\n","1. **División en Parches**: La imagen se divide en parches de $N \\times N$ píxeles.\n","   \n","2. **Generación de Embeddings**: Cada parche se transforma en un embedding que captura su información relevante.\n","\n","3. **Cifrado de Posición**: Se añade un vector de cifrado de posición a cada embedding.\n","\n","   Algunos ejemplos del cifrado:\n","\n","\n","   *  **Codificación Sinusoidal:** La fórmula sinusoidal para embebidos posicionales (tradicional en Transformers-NLP) genera los valores de los embebidos en función de senos y cosenos. Estos valores dependen directamente de la posición y siguen una periodicidad específica para capturar relaciones posicionales. Es determinista y no se ajusta durante el entrenamiento, ya que los valores están calculados y fijos para cada posición.\n","   Donde:\n","       - $pos$ es la posición en la secuencia.\n","       - $i$ es el índice de la dimensión.\n","       - $d_{model}$ es la dimensión del modelo.\n","\n","   $$PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$$\n","\n","   $$PE(pos, 2i + 1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$$\n","\n","\n","\n","   * **Embebido Posicional Aprendido (a ser implementada por el alumno):** los embebidos posicionales se definen como un parámetro de la red neuronal (usando [nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html)), lo que significa que los valores para el embebido  posicional se aprenden durante el proceso de entrenamiento. Se define embedding, matriz de tamaño (1, num_patches, embed_dim), donde num_patches es el número de parches (posiciones) en la secuencia de la imagen y  embed_dim es la dimensión de los embebidos. La inicialización torch.randn genera valores aleatorios para estos embebidos posicionales, y luego el modelo los optimiza durante el entrenamiento, ajustando los valores en función de los gradientes.\n","\n","4. **Combinación de Embeddings y Cifrado de Posición**: Se suma cada embedding con su correspondiente cifrado de posición, generando un vector final que contiene tanto la información del contenido del parche como su posición en la imagen.\n","\n","El resultado es una serie de vectores, cada uno representando un parche y su posición, que se alimentan a las capas del Transformer. Esto permite al modelo aprender no solo sobre las características individuales de cada parche, sino también sobre cómo estos se relacionan entre sí en el contexto de la imagen completa.\n","\n","\n","![Vision Transformer](vit.gif)\n","\n","*Crédito: [Phil Wang](https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif)*\n"]},{"cell_type":"markdown","metadata":{"id":"BYkwR_4L0Pr7"},"source":["A continuación, trabajaremos con los **patch embeddings** y la **codificación posicional**, esenciales para que el modelo pueda interpretar las relaciones espaciales en las imágenes.\n","\n","## Tareas:\n","\n","1. **Modificar los parametros:** Cambiar el tamaño de los parches, y la cantidad de dimensiones del embedding. Investigar y describir las ventajas y desventajas de tener más o menos parches/dimensiones.\n","\n","2. **Implementar embedding posicional aprendido 1D:** Partiendo de ejemplo proporcionado (codificación sinusoidal en clase PositionalEncoding), implementar una clase PositionalEncodingLearned que utilice PyTorch y genere embebidos posicionales aprendidos. Graficar.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"sVePqRby0Pr8","executionInfo":{"status":"ok","timestamp":1730325619220,"user_tz":180,"elapsed":6321,"user":{"displayName":"Sofia Speri","userId":"12363663285422232514"}}},"outputs":[],"source":["import torch\n","from torch import Tensor\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import math\n","\n","# La configuración, carga y preprocesamiento\n","class ConfigPreprocess:\n","    def __init__(self, img_path: str, img_size: int, patch_size: int):\n","        self.device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n","        print(f'Dispositivo utilizado: {self.device}')\n","\n","        self.img_path = img_path\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.test_img = self.load_image()\n","\n","    def load_image(self):\n","        return TF.to_tensor(Image.open(self.img_path).resize((self.img_size, self.img_size))).unsqueeze(0).to(self.device)\n","\n","    def extract_patches(self, image: Tensor) -> Tensor:\n","        patches = image.unfold(1, self.patch_size, self.patch_size).unfold(2, self.patch_size, self.patch_size)\n","        patches = patches.contiguous().view(image.shape[0], -1, self.patch_size, self.patch_size)\n","        return patches\n","\n","\n","class PatchEmbedding(nn.Module):\n","    def __init__(self, img_size: int, patch_size: int, in_channels: int = 3, embed_dim: int = 8):\n","        super(PatchEmbedding, self).__init__()\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = (img_size // patch_size) ** 2\n","        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, x):\n","        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n","        x = x.flatten(2)  # (B, embed_dim, num_patches)\n","        x = x.transpose(1, 2)  # (B, num_patches, embed_dim)\n","        return x\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, num_patches, embed_dim):\n","        super(PositionalEncoding, self).__init__()\n","        self.register_buffer('pos_embedding', self.create_positional_encoding(num_patches, embed_dim))\n","\n","    def create_positional_encoding(self, num_patches, embed_dim):\n","        position = torch.arange(num_patches, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n","        pos_encoding = torch.zeros(num_patches, embed_dim)\n","        pos_encoding[:, 0::2] = torch.sin(position * div_term)  # Aplicar seno en dimensiones pares\n","        pos_encoding[:, 1::2] = torch.cos(position * div_term)  # Aplicar coseno en dimensiones impares\n","        return pos_encoding.unsqueeze(0)  # Añadir dimensión de batch\n","\n","    def forward(self, x):\n","        return x + self.pos_embedding\n","\n","class Visualization:\n","    @staticmethod  # No requiere self\n","    def visualize_patches(patches: Tensor):\n","        num_patches = patches.shape[1]\n","        num_cols = int(num_patches ** 0.5)\n","        num_rows = (num_patches + num_cols - 1) // num_cols\n","        _, axs = plt.subplots(num_rows, num_cols, figsize=(4, 4))\n","        for i in range(num_rows):\n","            for j in range(num_cols):\n","                idx = i * num_cols + j\n","                if idx < num_patches:\n","                    patch = patches[:, idx]\n","                    num_channels = patch.shape[0]\n","                    if num_channels == 1:\n","                        axs[i, j].imshow(patch.squeeze().detach().cpu().numpy(), cmap='gray')\n","                    elif num_channels == 3:\n","                        axs[i, j].imshow(patch.permute(1, 2, 0).detach().cpu().numpy())\n","                    axs[i, j].axis('off')\n","                else:\n","                    axs[i, j].axis('off')\n","        plt.tight_layout(pad=0.15)\n","        plt.show()\n","\n","    @staticmethod  # No requiere self\n","    def visualize_positional_encoding(pos_embeddings: Tensor):\n","        plt.figure(figsize=(14, 4))\n","        plt.title('Codificaciones Posicionales para los Parches', fontsize=16, weight='bold')\n","        for i in range(pos_embeddings.shape[2]):\n","            plt.plot(pos_embeddings[0, :, i].detach().cpu().numpy(), label=f'Dimensión {i + 1}')\n","        plt.xlabel('Índice del Parche', fontsize=14)\n","        plt.ylabel('Valor de la Codificación Posicional', fontsize=14)\n","        plt.legend()\n","        plt.grid()\n","        plt.show()\n","\n","    @staticmethod\n","    def visualize_single_patch_encoding(pos_embeddings: Tensor, patch_idx: int):\n","        num_patches = pos_embeddings.shape[1]\n","        if patch_idx < 0 or patch_idx >= num_patches:\n","            raise ValueError(f\"El índice del parche debe estar entre 0 y {num_patches - 1}, pero se recibió {patch_idx}.\")\n","        patch_encoding = pos_embeddings[0, patch_idx, :].detach().cpu().numpy()\n","        plt.figure(figsize=(14, 4))\n","        plt.plot(patch_encoding, marker='o', label=f'Parche {patch_idx + 1}')\n","        plt.title(f'Encoding Posicional para el Parche {patch_idx + 1}', fontsize=16, weight='bold')\n","        plt.xlabel('Dimensión del Embedding', fontsize=14)\n","        plt.ylabel('Valor del Encoding', fontsize=14)\n","        plt.grid()\n","        plt.legend()\n","        plt.show()\n","\n","\n"]},{"cell_type":"markdown","source":["### 1\n","\n","Se van a hacer las siguientes pruebas:\n","* A. Patch_size 64 / Dimension de Emb = 8\n","* B. Patch_size 64 / Dimension de Emb = 16\n","* C. Patch_size 32 / Dimension de Emb = 16\n","* D. Patch_size 128 / Dimension de Emb = 16"],"metadata":{"id":"nUSBpIbp_3lh"}},{"cell_type":"code","source":["import time"],"metadata":{"id":"Aip3nRIndlFt","executionInfo":{"status":"ok","timestamp":1730325963943,"user_tz":180,"elapsed":376,"user":{"displayName":"Sofia Speri","userId":"12363663285422232514"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["patch_size_vec = [64,64,32,128]\n","embed_dim_vec = [8,16,16,16]"],"metadata":{"id":"ReDeJOGtVb_X","executionInfo":{"status":"ok","timestamp":1730325965941,"user_tz":180,"elapsed":7,"user":{"displayName":"Sofia Speri","userId":"12363663285422232514"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["for i in range(len(patch_size_vec)):\n","  print (f\"Graficos con patch_size = {patch_size_vec[i]} y embed_dim = {embed_dim_vec[i]}\")\n","  # Parámetros\n","  img_path = \"raccoon.jpg\"\n","  img_size = 900\n","  patch_size = patch_size_vec[i]\n","  embed_dim = embed_dim_vec[i]\n","  patch_idx = 0  # El índice del parche para el cual queres visualiizar la codificación posicional\n","\n","  start_time = time.time()\n","  # Preprocesamiento\n","  config = ConfigPreprocess(img_path, img_size, patch_size)\n","\n","  # Extracción de parches y visualización\n","  patches = config.extract_patches(config.test_img.squeeze(0))\n","  Visualization.visualize_patches(patches)\n","\n","  # Generación de embeddings\n","  embedded_patches = PatchEmbedding(img_size, patch_size, 3, embed_dim).to(config.device)\n","  patches = embedded_patches(config.test_img)\n","\n","  # Codificación posicional\n","  num_patches = (img_size // patch_size) ** 2\n","  positional_encoding = PositionalEncoding(num_patches, embed_dim).to(config.device)\n","  pos_embeddings = positional_encoding(patches)\n","  Visualization.visualize_positional_encoding(pos_embeddings)\n","  Visualization.visualize_single_patch_encoding(pos_embeddings, patch_idx)\n","\n","  end_time = time.time()\n","  execution_time = end_time - start_time\n","  print(f\"Tiempo de ejecución: {execution_time} segundos\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Rc_OlQKP4LOYxQ5XYmaP9E5VENjP-yB8"},"collapsed":true,"id":"_38TZVM2-7M_","executionInfo":{"status":"ok","timestamp":1730326007642,"user_tz":180,"elapsed":38981,"user":{"displayName":"Sofia Speri","userId":"12363663285422232514"}},"outputId":"719882f6-b7af-4bec-d0cb-1699a35e6026"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["De las pruebas podemos ver que al reducir el tamaño de los patches, aumenta la frecuencia de la codificación posicional. Eso puede deberse a que al dividir la imagen en parches mas pequeños, el PE tiene que diferenciar más posiciones en la  imagen, lo que requiere frecuencias más altas para poder representarlas.\n","\n","Por otro parte,en cuanto a la variación del tamaño del embedding en los dos primeros casos no modifica la frecuencia del PE pero si se puede notar un aumento en el tiempo de ejecución. Esto indica mayor costo computacional, atribuible a la capacidad de los embeddings más grandes para capturar mejor las características de la imagen y las relaciones entre los patches. Este aumento en dimensionalidad puede potencialmente mejorar la precisión del modelo.\n","\n","Entonces como análisis de la variación del tamaño de los patches podemos decir que cuanto más divido la imagen (es decir tamaño de patches más pequeños) capturo más características locales, pero genero más tokens para mi modelo. Para el caso de los embeddings, el tamaño de los mismos va a tambipen afectar en un punto en la capacidad computacioneal y por otro en la capacidad de los tokens de rpresentar patrones complejos. Por lo que el valor a definir en este caso dependerá del problema para pdoer tener tamaños de embeddings y patches que puedan cpturar la inofrmación importante de las imagenes y puedan ser operados computacionalmente."],"metadata":{"id":"RE1FGi-uZVlV"}},{"cell_type":"markdown","source":["### 2\n","\n","Implementación de clase PositionalEncodingLearned, que aprende los valores del positional encoding durante el entrenamiento gracias al uso de torch.nn.Parameter.\n","\n","Se realizan los mismos gráficos y casos que el Ejercicio 1.\n"],"metadata":{"id":"yzK1h0TxAScR"}},{"cell_type":"code","source":["class PositionalEncodingLearned(nn.Module):\n","    def __init__(self, num_patches, embed_dim):\n","        super(PositionalEncodingLearned, self).__init__()\n","        self.register_buffer('pos_embedding', self.create_positional_encoding(num_patches, embed_dim))\n","\n","    def create_positional_encoding(self, num_patches, embed_dim):\n","        position = torch.arange(num_patches, dtype=torch.float).unsqueeze(1)\n","        pos_encoding = torch.nn.Parameter(torch.randn(1, num_patches, embed_dim))\n","        return pos_encoding\n","\n","\n","    def forward(self, x):\n","        return x + self.pos_embedding"],"metadata":{"id":"8i3oWNpcAUTQ","executionInfo":{"status":"ok","timestamp":1730327227457,"user_tz":180,"elapsed":356,"user":{"displayName":"Sofia Speri","userId":"12363663285422232514"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["for i in range(len(patch_size_vec)):\n","  print (f\"Graficos con patch_size = {patch_size_vec[i]} y embed_dim = {embed_dim_vec[i]}\")\n","  # Parámetros\n","  img_path = \"raccoon.jpg\"\n","  img_size = 900\n","  patch_size = patch_size_vec[i]\n","  embed_dim = embed_dim_vec[i]\n","  patch_idx = 0  # El índice del parche para el cual queres visualiizar la codificación posicional\n","\n","  start_time = time.time()\n","  # Preprocesamiento\n","  config = ConfigPreprocess(img_path, img_size, patch_size)\n","\n","  # Extracción de parches y visualización\n","  patches = config.extract_patches(config.test_img.squeeze(0))\n","  Visualization.visualize_patches(patches)\n","\n","  # Generación de embeddings\n","  embedded_patches = PatchEmbedding(img_size, patch_size, 3, embed_dim).to(config.device)\n","  patches = embedded_patches(config.test_img)\n","\n","  # Codificación posicional\n","  num_patches = (img_size // patch_size) ** 2\n","  positional_encoding = PositionalEncodingLearned(num_patches, embed_dim).to(config.device)\n","  pos_embeddings = positional_encoding(patches)\n","  Visualization.visualize_positional_encoding(pos_embeddings)\n","  Visualization.visualize_single_patch_encoding(pos_embeddings, patch_idx)\n","\n","  end_time = time.time()\n","  execution_time = end_time - start_time\n","  print(f\"Tiempo de ejecución: {execution_time} segundos\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1_rzHeUD_VyNI8-AoGMjcKXRb1zzhLY1x"},"collapsed":true,"id":"ncfDRxAyNYh8","executionInfo":{"status":"ok","timestamp":1730327332513,"user_tz":180,"elapsed":41589,"user":{"displayName":"Sofia Speri","userId":"12363663285422232514"}},"outputId":"4678dfd1-7db5-479e-e0e4-46f7972aa29d"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["\n","La ventaja de los Learned Positional Encoders vs. la codificación siusoidal es que al no ser fijo, y aprender en cada iteración; se adapta mejor a los datos específicos de la imagen siendo más flexibles cuando las secuencias varían en longituss o existen patrones inusuales.\n","\n","En el caso de este ejemplo, los tiempos de ejecución entre ambos embeddigns fueron del mismo orden."],"metadata":{"id":"BAwicki5i4xY"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}